from PIL import Image
import numpy as np
import os as os
import matplotlib.pyplot as plt
import collections
from collections import Counter
from scipy.fftpack import dct
from skimage.io import imread
from skimage.color import rgb2gray
from nltk.util import ngrams
import networkx as net
import numpy as np
from numpy import random
import math
import pandas as pd
import cv2
from itertools import islice
from tqdm import tqdm
import pandas as pd
import sklearn as sk
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import AdaBoostClassifier
from sklearn import metrics

%matplotlib inline
import re
import os
# def con_and_save(arr, name):
#   print('Processing ' + name)
#   if arr.shape[1] != 16: # not hex
#     assert(False)
#   b=int((arr.shape[0]*16)**(0.5))   
#   b=2**(int(np.log(b)/np.log(2))+1) 
#   a=int(arr.shape[0]*16/b)
#   arr=arr[:a*b//16,:]
#   arr=np.reshape(arr,(a,b))
#   im=Image.fromarray(np.uint8(arr))
#   im.save('/content/image/' + name + '.png', "PNG")
#   return im

# def wordvec(op):
  
def extractop(op,name):
    
    li = ['db', 'fdivrp', 'fstp', 'div', 'ja', 'pop', 'jnz', 'and', 'movsx', 'jnb', 'byte', 'eax', 'call', 'fstcw', 'CC', 'not', 'shl', 'setz', 'jns', 'imul', 'stmxcsr', 'jl', 'lea', 'jle', 'mov', 'xor', 'jz', 'mul', 'cmp', 'sar', 'movsd', 'fnclex', 'fldcw', 'edx', 'nop', 'offset', 'jg', 'ecx', 'leave', 'push', 'ebx', 'stosd', 'wait', 'fistp', 'esp', 'movzx', 'sub', 'sbb', 'fnstsw', 'align', 'inc', 'or', 'cdq', 'test', 'jb', 'esi', 'jbe', 'edi', 'jmp', 'fldpi', 'retn', 'dword', 'fldz', 'ldmxcsr', 'fld1', 'fld', 'setnle', 'fstsw', 'neg', 'rep', 'jge', 'setnz', 'shr', 'retf', 'dec', 'add', 'xchg', 'ror', 'rol', 'rtn']
    # op = list()
    with open(name,'r',encoding = "utf-8",errors="ignore") as fin:
        for i in fin:
            s = i.split(' ')
            for k in s:
                if k in li:
                  op.append(k)
    # print(op)
    return op   

def countbyte(name):      
  count = 0
  byte = list()
  # bytecount = 42*42 
  # print(bytecount)
  with open(name,'r') as fin:
        # while count<= bytecount*100:
          for line in fin:
            xx=line.split()
            for i in xx[1:]:
                  if count == (256*256):
                    break
                  elif i !='??':
                    byte.append(int(i,16))
                    
                    # print(byte.append(int(i,16)))
                  else:
                    byte.append(0)
                  count += 1
                 
  # print('byte len',len(byte))
  return byte


def pagetext(res_dct):
  with open("/content/asmgraph.txt",'w') as fout:
    for key,value in res_dct.items():
      fout.write(key+" "+value)
      fout.write("\n")
    fout.close()
  return fout   


def pagerank():
  FielName='/content/asmgraph.txt'
  Graphtype=net.DiGraph()  
  G = net.read_edgelist(
      FielName, 
      create_using=Graphtype,
      nodetype=str,
      data=(('weight',int),1)
  )     
  pr = net.pagerank(G, alpha=0.9,max_iter=1000 )  
  return pr

def pr_convert(op,pr):
  with open('pageopcode.txt', 'w+') as newfile:
   for i in op:
      if i in pr.keys():
        # newfile.write(line.replace(word,'%f'%value))
        newfile.write('%f'%pr.get(i))
        newfile.write(" ")

def page_list():
  pagelist = list()
  count = 0
  with open('pageopcode.txt','r') as file:
    for line in file:
          # for word, value in pr.items():
          for value in line.split():
            if(count == 80*80):
              break
            else:
              pagelist.append(value)
              count += 1

  # print(pagelist)
  # print(len(pagelist))
  return pagelist


def normalize(pagelist):
  # print(pagelist)
  page_len = len(pagelist)
  # print('pagelist len',page_len)
  page_len = int(math.sqrt(page_len))
  newarray = list()
  for i in range(page_len*page_len):
    newarray.append(i)

  pagearray = np.reshape(newarray,(page_len,page_len)).astype(np.float)
  # print(pagearray)
  try:
    pagearray = pagearray - pagearray.min()
    pagearray = pagearray/pagearray.max()
    pagearray = (pagearray*255).astype(np.uint8)
    # print(pagearray)
  except ValueError:
    pass  
  return pagearray

def mat_convert(array):
    if(len(array)) < 256:
      hori_array = np.zeros((256-len(array),len(array[0])),dtype = int)
      res_array = np.concatenate((array,hori_array),axis = 0)
    if(len(array[0])) < 256:
      vert_array = np.zeros((256,(256-len(array[0]))),dtype = int)
      res_array = np.concatenate((res_array,vert_array),axis = 1)
    # print('res_array',res_array)
    return res_array


def direct_conv():
      directory = '$SCRATCH/msbig/train/'
      files = os.listdir(directory)
      #print(files[0].split("."))
      sorted_files = sorted(files)
      bytecount=0
      count = 0
      for i in range(0, len(sorted_files), 2):
           print(sorted_files[i])
           asmfile = directory+"/"+sorted_files[i]
           bytefile = directory+"/"+sorted_files[i+1] 
           op = list()
           opcode = extractop(op,asmfile)
          #  print('opcodelen',len(opcode))
           array = list()
           res_dct = {op[i]: op[i + 1] for i in range(0, len(op)-1, 1)}
          #  print(res_dct)
          #  print(len(res_dct))
          #  print(opcode)
           if (len(opcode)>0):
              pagetext(res_dct)
              pr = pagerank()
              pr_convert(op,pr)
              pagelist = page_list()
              #  print(page_list)
              #  print(pagelist)
              page_array = normalize(pagelist)
              #  print(page_array.shape)
              #  print(page_array)
              im_arr = mat_convert(page_array)
              n = len(page_array)
              byte_list = list()
              n1 = len(pagelist)
              byte_array = countbyte(bytefile) 
              # print(len(byte_array)) 
              if(len(byte_array) < 65536):
                n = 65536-len(byte_array)
                while(n > 0):
                  byte_array.append(0)
                  n -= 1
              byte_array1 = np.reshape(byte_array,(256,256))
              final = np.dstack([im_arr,byte_array1])
              if len(final) > 0:
                im = Image.fromarray((final).astype(np.uint8))   
                im.save('/content/drive/MyDrive/grayimages/' + sorted_files[i] + '.png', "PNG")
                count += 1
                print(count)
           else:
              continue
         
def compute_hist(x,bins=256):
  h_arr = np.histogram(x,bins=bins)[0]
  return h_arr/np.linalg.norm(h_arr)

def hist_blocks(test_image,bins=256,windowsize_r=32,windowsize_c=32, stride_factor=1):
 
# Define the window size
  #windowsize_r = 32
  #windowsize_c = 32

  hist_array = []
# Crop out the window and calculate the histogram
  for r in range(0,test_image.shape[0], int(windowsize_r/stride_factor)):
    for c in range(0,test_image.shape[1], int(windowsize_c/stride_factor)):              #test_image.shape[0] -> [0] accounts for dimension in x axis, [1] for y axis
      window = test_image[r:r+windowsize_r,c:c+windowsize_c]
      #hist = np.histogram(window,bins=range(0,260,4))[0]
      hist = compute_hist(window,bins=bins)
      hist_array.append(hist)
  
  return np.array(hist_array).reshape(-1)


def extract_runner(bin_width, group_row, group_col, _start, _stop, r_win, c_win,sf):
  files = os.listdir(DCT_LOC)
  files.sort()
  meta_dic = {"Id": [], "array": []}
  for counter, name in enumerate(islice(files, _start, _stop), _start):
    meta_dic["Id"].append(name)
  #feature_set = []
  for counter, name in tqdm(enumerate(islice(files, _start, _stop), _start)):
    img_hist = Image.open(DCT_LOC + '/' + name)
    img_array = np.asarray(img_hist)

    res = hist_blocks(img_array, windowsize_c=c_win, windowsize_r=r_win, stride_factor=sf)
    #res2 =list(res)
    #feature_set.append(res)
    meta_dic["array"].append(res)
  return meta_dic








if __name__ == '__main__':
    direct_conv()

    one_file = os.listdir(DCT_LOC)
    one_file = min(one_file)
    image_for_hist = Image.open(DCT_LOC + '/' + one_file)
    image_array = np.asarray(image_for_hist)

    res_hist = hist_blocks(image_array)
    print(res_hist.shape)

    print(res_hist)
    plt.plot(res_hist)

    feature_1 = extract_runner(0, 0, 0, 0, 4095, 64, 64,1)

    print(len(feature_1))
    len(feature_1['array'][0])

    yyy = []
    xxx = []
    for i in range(256):
    yyy.append(feature_1['array'][0][i])
    xxx.append(i)

    plt.hist(yyy, bins=256)
    plt.show
    labels = pd.read_csv(TRAIN_LABEL)
    sorted_labels = labels.sort_values(by=["Id"], ascending = True)
    sorted_labels

    train_feature = pd.DataFrame(feature_1)
    tf2 = pd.DataFrame(train_feature['array'].to_list())  #ETF: 3min
    tf3 = pd.concat([train_feature, tf2], axis=1)
    tf_final = tf3.drop(columns=['array'])

    tf_use = tf_final.drop(columns=['Id'])
    label_use = sorted_labels.drop(columns=['Id'])

    X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(tf_use, label_use, test_size=0.2, random_state=47)
    print(X_train.shape, y_train.shape)

    ############### RANDOM FOREST ################
    
    rfc = sk.ensemble.RandomForestClassifier(n_estimators=100)
    rfc.fit(X_train,y_train)
    y_pred_rfc = rfc.predict(X_test)
    print(sk.metrics.classification_report(y_test, y_pred_rfc))

    ############### XGBoost ######################

    xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 100)
    xg_reg.fit(X_train,y_train)
    preds = xg_reg.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, preds))
    print("RMSE: %f" % (rmse))

    ################ AdaBoost ####################
    
    abc = AdaBoostClassifier(n_estimators=100,learning_rate=0.1)
    model = abc.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    
    print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
    


